
In this section we propose a method for jointly training two word alignment models, a source-to-target model $\TA$ and a target-to-source model $\TB$, by regularizing their parameters to respect the invertibility of the alignment task. 
We therefore name our method \emph{Model Invertibility Regularization} (\method{MIR}).

\subsection{Regularizer}

Our regularizer operates on the t-table parameters $\tA, \tB$ of the two models, as follows:
Let matrices $T_1, T_2$ denote the t-tables $\tA, \tB$ in matrix form and consider their multiplication $T=T_1 T_2$. 
The resulting matrix $T$ is a stochastic square matrix of dimension $|V_1|\times|V_1|$ where $|V_1|$ is the size of the source-language vocabulary.
Each entry $T_{ij}$ represents the total probability mass mapped from source word $e_i$ to source word $e_j$ by first applying the source-to-target mapping $T_1$ and then the target-to-source mapping $T_2$.

In particular, each diagonal entry $T_{ii}$ holds the probability of mapping a source symbol back onto itself, a quantity we intuitively believe should be high.
We therefore (initially) consider maximizing the trace of~$T$:
\begin{equation*}
Tr[T]=\sum_i T_{ii}=\sum_{e}\sum_f t_{1}(f\mid e)\; t_{2}(e\mid f)
\end{equation*}
We further note that~$Tr[T]=Tr[T_{1}T_{2}]=Tr[T_{2}T_{1}]$, so that the trace captures equally well how much the target symbols map onto themselves.

Since $T$ is stochastic, setting it to the identity matrix $I$ maximizes its trace. In other words, the more $T_1$ and $T_2$ behave as (pseudo-)inverses of each other, the higher the trace is. This exactly fits with our intuition regarding invertibility.

Unfortunately, the trace is not concave in both $T_1$ and $T_2$, a property which will become desirable in optimization.
We therefore modify the trace regularizer by applying the entrywise square root operator on $T_1,\,T_2$ and denote the new term $R$:
\begin{align}
R(t_{1},t_{2})&=Tr[\sqrt{T1}\sqrt{T2}]\nonumber\\
&=\sum_{e,f}\sqrt{t_{1}(f\mid e) \; t_{2}(e\mid f)}
\label{eqn:R}
\end{align}
Note that $R$ is also maximized whenever $\sqrt{T1}\sqrt{T2}=I$.

Concavity of $R$ in both $t_1, t_2$ (or equivalently $T_1, T_2$) follows by observing that it is a sum of concave functions -- each term in the summation is a geometric mean, which is concave in its parameters. 



%----- VERION 1 

\subsection{Joint objective function}

We apply \method{MIR} in two data scenarios:
In the parallel data setting, we observe $N$ sequences pairs $\set{\vx^n_1}_{n}=\set{(\ve^n, \vf^n)}_{n}$ or, equivalently, $\set{\vx^n_2}_{n}=\set{(\vf^n, \ve^n)}_{n}$.

In the non-parallel setting, two monolingual datasets are observed -- $N_1$ source sequences $\set{\vx^n_1}_{n}=\set{\ve^n}_{n}$ and $N_2$ target sequences $\set{\vx^n_2}_{n}=\set{\vf^n}_{n}$.

The probability of the $n$th sample under the $k$th ($k\in\set{1,2}$) model $\Tk$ is denoted $\pk(\vx_k^n; \Tk)$.
Specifically, in the parallel data setting, the probability of $\vx_k^n$ under its model is\footnote{This slight notation abuse helps represent both data scenarios succinctly.}:
\begin{align*}
\pA(\vx_1^n; \TA) = p(\vf^n \mid \ve^n; \TA)\\
\pB(\vx_2^n; \TB) = p(\ve^n \mid \vf^n; \TB)
\end{align*}
whereas in the non-parallel data setting, the probability is defined:
\begin{align*}
\pA(\vx_1^n; \TA) = p(\vf^n; \TA)\\
\pB(\vx_2^n; \TB) = p(\ve^n; \TB)
\end{align*}
\marginpar{DC: I feel that the x notation is over-complex. Would it help to not try to unify the parallel and non-parallel cases?}

Using the above definitions and the \method{MIR} regularizer $R$ (Eq. \ref{eqn:R}), we formulate an optimization program for maximizing the regularized log-likelihoods of the observed data:
\marginpar{DC: The use of symbols $t$ and $\Theta$ for nearly the same thing can be confusing}
\begin{equation}
\max_{\TA, \TB}\,\, \lambda R(t_1, t_2)
%+\sum_{k\in\set{1,2}} \log \sum_n \pk(\vx^n; \Tk)
%+\sum_{k\in\set{1,2}} L_k(\set{\vx_k^n}; \Tk)
+\sum_{k\in\set{1,2}} \sum_{n=1}^{N_k} \log p_k(\vx_k^n; \Tk)
\label{eqn:joint}
\end{equation}
where $\lambda\ge0$ is a tunable hyperparameter and, in the parallel case $N=N_1=N_2$.

We defer the discussion on the relationship and merits of our approach with respect to \method{ABA} \cite{liang+:2006:align} to Section \ref{sec:related}.

%------ VERSION 2 (new)
%
%\subsection{Joint objective function}
%We consider two data scenarios for \method{MIR}. For each setting we formulate an optimization program that maximizes the regularized data log-likelihoods using our regularizer $R$ (\eqn{eqn:R}). In both programs, $\lambda\ge0$ denotes a tunable hyperparameter.
%
%In the parallel data scenario, we observe samples $\set{(\ve^n, \vf^n)}_{n=1}^N$ and maximize:
%\begin{align}
%\max_{\TA, \TB}\,\, &\lambda R(t_1, t_2)
%+ \sum_n \log p_1(\vf^n \mid \ve^n)\\
%&+ \sum_n \log p_2(\ve^n \mid \vf^n)\nonumber
%\label{eqn:joint_parallel}
%\end{align}
%
%In the non-parallel setting, two monolingual datasets are observed $\set{\ve^n}_{n=1}^{N_1}$ and $\set{\vf^n}_{n=1}^{N_2}$ and we maximize
%\begin{align}
%\max_{\TA, \TB}\,\, &\lambda R(t_1, t_2)
%+ \sum_n \log p_1(\vf^n; \TA)\\
%&+ \sum_n \log p_2(\ve^n; \TB)\nonumber
%\label{eqn:joint_mono}
%\end{align}
%
%We defer the discussion on the relationship and merits of our approach with respect to \method{ABA} \cite{liang+:2006:align} to Section \ref{sec:related}.
%
%-- END





%\subsection{remove me}
%In word alignment, it is common practice to train alignment models in both source-to-target and target-to-source directions and then apply symmetrization techniques to the resulting source-to-target and target-to-source alignments. 
%%Alignment symmetrization heuristics, such as intersection, union or grow-diag-final-and play a significant role in reducing alignment errors and ultimately improving translation quality.
%The improvement gained by these post-processing heuristics suggest that each (directional) model suffers from different errors, and that those errors can be mitigated by symmetrization.
%
%Observing this, \newcite{liang+:2006:align} formulate Alignment by Agreement, which maximizes both the observed data-likelihoods as well as a measure of agreement between the two models' posteriors.
%Showing significant reductions in AER, they demonstrate the effectiveness of jointly learning the models. However, because both symmetrization heuristics and Alignment by Agreement try to create agreement between the inferences of the two models, they necessarily run the two models on the same data, which must be parallel.
%
%In this section, we describe Parameter Agreement Training (PAT), a method for jointly learning directional models that does not depend on parallel data. 	
%We follow Liang et al. (2006) in formulating a joint objective function for training the two directional alignment models, but define the agreement measure in a way that can be applied either to the parallel data or non-parallel data setting.
%
%In the parallel data setting, we observe $N$ paired sequences $\set{\vx^n_1}_{n=1}^N=\set{(\ve^n, \vf^n)}_{n=1}^N$ or, equivalently, $\set{\vx^n_2}_{n=1}^N=\set{(\vf^n, \ve^n)}_{n=1}^N$.
%
%In the non-parallel data setting, we observe two sets of data points $\set{\vx^n_1}_{n=1}^{N_1} = \set{\ve^n}_{n=1}^{N_1}$ and $\set{\vx^n_2}_{n=2}^{N_2} = \set{\vf^n}_{n=1}^{N_2}$.
%
%Let $\TA$ and $\TB$ represent the two directional models' parameters.
%For $k\in\set{1,2}$, the probability of the $n$th sample under the $k$th model is denoted $\pk(\vx_k^n; \Tk)$.
%Specifically, in the parallel data setting, the directional probability of $\vx_k^n$ under its model is:
%\begin{align*}
%\pA(\vx_1^n; \TA) = p(\vf^n \mid \ve^n; \TA)\\
%\pB(\vx_2^n; \TB) = p(\ve^n \mid \vf^n; \TB)
%\end{align*}
%whereas in the non-parallel data setting, the probability is defined:
%\begin{align*}
%\pA(\vx_1^n; \TA) = p(\vf^n; \TA)\\
%\pB(\vx_2^n; \TB) = p(\ve^n; \TB)
%\end{align*}
%
%In either case, independently optimizing each model entails the maximization of its data log-likelihood (for $k\in{1,2}$):
%$L_k(\set{\vx^n_k}; \Tk)=\sum_n \log\pk(\vx_k^n; \Tk)$
%%
%%\begin{align*}
%%\max_{\Tk}\,\,L_k(\set{\vx^n_k}; \Tk)= 
%%\max_{\Tk}\,\,  \sum_n \log \pk(\vx_k^n; \Tk)
%%\end{align*}
%
%To jointly optimize the two models, a coupling term $R(\TA, \TB)$ is introduced, which encodes a measure of agreement between the two models.
%The resulting joint optimization problem takes the following form:
%\begin{equation}
%\max_{\TA, \TB}\,\, \lambda R(\TA, \TB)
%%+\sum_{k\in\set{1,2}} \log \sum_n \pk(\vx^n; \Tk)
%+\sum_{k\in\set{1,2}} L_k(\set{\vx_k^n}; \Tk)
%\label{eqn:joint}
%\end{equation}
%with $\lambda\ge0$ being a tunable hyperparameter.
%
%\subsection{Parameter Agreement Measure}
%Both the word alignment and transliteration models presented in Section \ref{sec:background} are parameterized by a translation table.
%Considering these tables in both directions, $t_1$ and $t_2$, we suggest the following parameter agreement measure: 
%\begin{equation}
%R(\TA, \TB) = \sum_{e,f} \sqrt{t_1(f \mid e) \cdot t_2(e \mid f)}
%\label{eqn:R}
%\end{equation}
%where $\ve$ and $\vf$ range over all source and target entity types.
%Note that $R$ disregards the distortion parameters $a$ (if at all present).
%
%Our agreement measure has several appealing properties.
%Using the Cauchy-Schwarz inequality, we can bound $R(\TA, \TB) \le \sqrt{|V_E|\cdot|V_F|}$ where $|V_E|, |V_F|$ denote the source and target vocabulary size.
%%\begin{align*}
%%R(\TA, \TB) 
%%%\sum_{e,f} \sqrt{(t_1(\ve \mid \vf) \cdot t_2(\vf \mid \ve))} 
%%\le &\sqrt{ (\sum_{e,f} t_1(\vf \mid \ve)) (\sum_{e,f} t_2(\ve \mid \vf))}\\
%%= &\sqrt{(|V_E|\cdot|V_F|)}
%%\end{align*}
%In particular, when $|V_E|=|V_F|$, the maximum is attained on parameter configurations for which $t_1(\vf \mid \ve) = t_2(\ve \mid \vf)$ over all entity pairs.
%% demonstrating that this agreement term is not biased towards any particular property (e.g, sparsity).
%Thus, the objective in \eqn{eqn:joint} balances between similarity in parameter magnitude and data likelihood. 
%%\marginpar{\bluetext{TL: this seems like a good place to say something about invertibility}}
%
%Another appealing property of $R$ is its concavity, which follows since $R$ is the sum of concave functions $h(x, y)=\sqrt{xy}$ over a closed convex set (see the concavity proof for $h$ in the appendix).
%
%% TODO: other agreement terms
%
%%We now describe our method, which we call Parameter Agreement Training.
%%In translation, we believe that if a word $e$ can be translated
%%to a word $f$, then $f$ can be translated to~$e$. 
%%This intuition holds for other tasks such as word or phoneme alignment.
%%Thus, a desirable form of model agreement is parameter
%%sparsity agreement: 
%%\begin{align*}
%%P_{0}(f\mid e) = 0 \text{ iff } P_{1}(e\mid f)=0.
%%\end{align*}
%%%A possibly simpler form of agreement is $P_{0}(e\,,f) = P_{1}(f,\: e)$ which encodes
%%% equality in joint distributions.
%
%%\subsection{Regularization}
%%To encourage this form of parameter agreement we add a regularizer that couples the two models together:
%%\begin{align}
%%\max_{\PP\,,\QQ}&\,\,L(\PP | X)+L(\QQ | X)+\lambda R(\PP,\, \QQ)
%%\end{align}
%%where $\lambda\ge0$ is a regularization coefficient to be tuned and
%%\begin{align}
%%R(\PP, \QQ) = \sum_{e,f}\sqrt{\PP(f\mid e)\QQ(e\mid f)} .
%%\end{align}
%%This regularizer has two attractive properties:
%%\begin{itemize}
%%\item It is concave on the probability simplex which lends to an efficiently solvable convex optimization program when the log-likelihood terms are themselves concave.
%
%%Concavity proof: 
%%The regularizer is a sum of concave functions defined over a convex set. 
%%each summand is concave  the region $[0,1]^{2}$ and the sum of concave functions is concave)
%%\item It encourages parameter sparsity agreement - If $\PP(e \mid f) = 0$ but $\QQ(f\mid e) > 0$, shifting weight away from $\QQ(f \mid e)$ could only increase the regularizer value.
%%\end{itemize}
%%In practice, we found the following two regularizers useful:`
%%\begin{align*}
%%R_{GM}(P_{0},\, P_{1}) & =  \sum_{e,f}\sqrt{P_{0}(f\mid e)\cdot P_{1}(e\mid f)}\\
%%%R_{CD}(P_{0},\, P_{1}) & =  -\frac{1}{2}\sum_{e,f}(P_{0}(e\mid f)-P_{1}(f\mid e))^{2}
%%\end{align*}
%%Where \emph{GM }stands for 'geometric mean' and \emph{CD} for 'conditional
%%difference'. 
%%
%%Note that both regularizers are concave on the probability simplex
%%- each term in their summation is concave over the region $[0,1]^{2}$,
%%and the sum of concave functions is concave. One difference between
%%$R_{GM}$ and $R_{CD}$ is that, while both regularizers attain higher
%%values when the conditional distributions agree, $R_{GM}$ attains
%%its maximum when those distributions are sparse (need example?)

\subsection{Optimization procedure}\label{subsec:optimization}
Using our concave regularizer, \method{MIR} optimization (\eqn{eqn:joint}) neatly falls under the MAP-EM framework \cite{Dempster:1977}
%\marginpar{insert reference to MAP-EM. \bluetext{TL: mentioned in Dempster}}
 and inherits the convergence properties of the underlying algorithms.
%
MAP-EM follows the same structure as standard EM - The E step remains identical to the standard E step; the M step maximizes the complete-data log-likelihood plus the regularization term.
%
In the case of \method{MIR}, the E-step can be carried out independently for each model. 
%In this sense, $\method{MIR}$ is orthogonal to the complexity of the models' inference space.
The only extra work is in the M-step, which optimizes a single (concave) objective function.

%Specifically, in the E-step, each model $\Tk$ (for $k\in\set{1,2}$) is held fixed and its posterior distribution over the missing data $\vz_k^n$\marginpar{DC: z appears out of the blue \bluetext{TBD}} is computed per each observation, $\vx_k^n$:
%$$q_k(\vz\kn, \vx\kn) \mathrel{:=} \pk(\vz\kn \mid \vx\kn ; \Tk)$$
%%\textbf{parallel data}: q_k(\va \mid \ve, \vf) :=& \pA(\va \mid \ve, \vf; \TA) \\
%%\textbf{decipherment}: q_1(\va, \ve \mid \vf) :=& \pA(\va, \ve \mid \vf; \TA)
%where, in the parallel data setting, only the alignment is missing ($\vz\kn = \va\kn$) and in the non-parallel data setting, both alignment and source symbol are missing
%($\vz_1^n = (\va_1^n, \ve^n), \vz_2^n = (\va_2^n, \vf^n)$).
%\\---\\
Specifically, let $\vz_n$ denote the missing data, where, in the parallel data setting, only the alignment is missing ($\vz\kn = \va\kn$) and in the non-parallel data setting, both alignment and source symbol are missing
($\vz_1^n = (\va_1^n, \ve^n), \vz_2^n = (\va_2^n, \vf^n)$).

In the E-step, each model $\Tk$ ($k\in\set{1,2}$) is held fixed and its posterior distribution over the missing data $\vz_k^n$
%\marginpar{DC: z appears out of the blue \bluetext{TL: better now?}} 
is computed per each observation, $\vx_k^n$:
$$q_k(\vz\kn, \vx\kn) \mathrel{:=} \pk(\vz\kn \mid \vx\kn ; \Tk)$$
%\textbf{parallel data}: q_k(\va \mid \ve, \vf) :=& \pA(\va \mid \ve, \vf; \TA) \\
%\textbf{decipherment}: q_1(\va, \ve \mid \vf) :=& \pA(\va, \ve \mid \vf; \TA)
%\\----\\

In the M-step, the computed posteriors are used to define a convex optimization program that maximizes the regularized sum of expected complete-data log-likelihoods:
 %$\TA, \TB$:
\begin{align*}
%(\TA', \TB') 
\max_{\TA, \TB}
\lambda R(t_1,t_2)
+
\sum_{k\in\set{1,2}} \sum_{n=1}^{N_k}  q_k(\vz\kn, \vx\kn) \log p_k(\vx\kn, \vz\kn)
\end{align*}
where $n$ ranges over the appropriate sample set.
%\marginpar{DC: two different $n$'s can be misleading, \bluetext{added $N_k$ here and above}}

Operationally, for models $\Tk$ that can be encoded as wFSTs (such as the IBM and HMM word alignment models), the E-step 
%posteriors $q_k$ 
can be carried out efficiently and exactly using dynamic programming \cite{Eisner02}. 
Other models resort to approximation techniques -- for example, the fertility-based word alignment models apply hill-climbing and sampling heuristics in order to efficiently estimate the posteriors \cite{brown+alii:1993}

From the computed posteriors $q_k$ we collect expected counts for each event, used to construct the M-step optimization objective. 
Since the \method{MIR} regularizer couples only the t-table parameters, the update rule for any remaining parameter is left unchanged (that is, one can use the usual closed-form count-and-divide solution).

Now, let $\Ch_1^{e,f} $ and $\Ch_2^{e,f}$ denote the expected counts for the t-table parameters. That is, $\Ch_k^{e,f}$ denotes the expected number of times a source-symbol type $e$ is seen aligned to a target-symbol type $f$ according to the posterior $q_k$. 
In the M-step, we maximize the following objective with respect to $t_1$ and  $t_2$:\marginpar{DC: This seems exactly the same as the previous equation -- why the new notation?}
\begin{align}
\arg\max_{t_1, t_2}\sum_{e,f}  \Ch_1^{e,f}\log t_1(f \mid e) +\nonumber \\
\sum_{e,f} \Ch_2^{e,f} \log t_2(e \mid f) + 
\lambda R(t_1, t_2) 
\label{eqn:Mobj}
\end{align}
which can be efficiently solved using convex programming techniques due to the   concavity of $R$ and the complete-data log-likelihoods in both $t_1$ and $t_2$.

\marginpar{cite Schoenemann, Vaswani? \bluetext{TL: revealing our identity?}}
To solve \eqn{eqn:Mobj} we applied Projected Gradient Descent \cite{bertsekas1999nonlinear}, where at each step, the parameters are updated in the direction of the M-step objective gradient at $(t_1, t_2)$ and then projected back onto the probability simplex. 
We used simple stopping conditions based on objective function value convergence and a bounded number of iterations.

%
%\iffalse
%\subsection{MIR for Word Alignment}
%As described in section~\ref{subsec:optimization}, in the E-step, we compute the posterior distribution over alignments in each direction, which amounts to computing the expected counts  $\E_1[C(e,f)] $ and $\E_2[C(e,f)] $, where $C(e,f)$ is the number of times $e$ is seen aligned to $f$. 
%In the M-step, we use the expected counts to re-estimate model parameters by maximizing the coupled expected complete data log likelihood. 
%In our work, we only encourage agreement between the translation probabilities $t(e \mid f)$ and $t(f \mid e)$. 
%Thus, the M-step objective function with respect to the translation probabilities is:
%\begin{align*}
%&\sum_{e,f}  \E_1[C(e,f)] \log t(e \mid f)  + {}\\ 
%&\quad \E_2[C(e,f)] \log t(f \mid e) + \sqrt{t(e \mid f)t(f \mid e)}, 
%\end{align*}
%which is maximized with projected gradient descent. 
%
%The estimation of the distortion probabilities is thus left unchanged (count and divide). We leave PAT of the distortion parameters for future work. 
%\fi


%At each time step, the models $\PP, \QQ$ are updated according to the gradient of the regularized objective function 
%%$\frac{\partial}{\partial \PP}(L(\PP \mid X)+\lambda R(\PP, \QQ)$
%and projected back onto the probability simplex. 
%
%Inspecting the regularizer's gradient we see that each update brings the two models closer:
%\begin{align*}
%\frac{\partial R}{\partial t_1(\vf\mid\ve)}
%&=\sqrt{\frac{t_2(\ve\mid\vf)}{2 t_1(\vf\mid\ve)}} \\
%\frac{\partial R}{\partial t_2(\ve\mid\vf)}
%&=\sqrt{\frac{t_1(\vf\mid\ve)}{2 t_2(\ve\mid\vf)}}
%\end{align*}
%%
%With symmetric terms for the gradient in $\QQ$. Note that the square root is taken entry-wise.


%\subsection{Agreement Training}
%In word alignment, it is common practice to train alignment models in both source-to-target and target-to-source directions and then apply symmetrization techniques to the resulting source-to-target and target-to-source alignments. 
%%Alignment symmetrization heuristics, such as intersection, union or grow-diag-final-and play a significant role in reducing alignment errors and ultimately improving translation quality.
%The improvement gained by these post-processing heuristics suggest that each (directional) model suffers from different errors, and that those errors can be mitigated by symmetrization.
%
%Observing this, \newcite{liang+:2006:align} formulate Alignment by Agreement, which maximizes both the observed data-likelihoods as well as a measure of agreement between the two models' posteriors.
%Showing significant reductions in AER, they demonstrate the effectiveness of jointly learning the models. However, because both symmetrization heuristics and Alignment by Agreement try to create agreement between the inferences of the two models, they necessarily run the two models on the same data, which must be parallel.
%
%In this section, we describe Parameter Agreement Training (PAT), a method for jointly learning directional models that does not depend on parallel data. 	
%We follow Liang et al. (2006) in formulating a joint objective function for training the two directional alignment models, but define the agreement measure in a way that can be applied either to the parallel data or non-parallel data setting.
%
%In the parallel data setting, we observe $N$ paired sequences $\set{\vx^n_1}_{n=1}^N=\set{(\ve^n, \vf^n)}_{n=1}^N$ or, equivalently, $\set{\vx^n_2}_{n=1}^N=\set{(\vf^n, \ve^n)}_{n=1}^N$.
%
%In the non-parallel data setting, we observe two sets of data points $\set{\vx^n_1}_{n=1}^{N_1} = \set{\ve^n}_{n=1}^{N_1}$ and $\set{\vx^n_2}_{n=2}^{N_2} = \set{\vf^n}_{n=1}^{N_2}$.
%
%Let $\TA$ and $\TB$ represent the two directional models' parameters.
%For $k\in\set{1,2}$, the probability of the $n$th sample under the $k$th model is denoted $\pk(\vx_k^n; \Tk)$.
%Specifically, in the parallel data setting, the directional probability of $\vx_k^n$ under its model is:
%\begin{align*}
%\pA(\vx_1^n; \TA) = p(\vf^n \mid \ve^n; \TA)\\
%\pB(\vx_2^n; \TB) = p(\ve^n \mid \vf^n; \TB)
%\end{align*}
%whereas in the non-parallel data setting, the probability is defined:
%\begin{align*}
%\pA(\vx_1^n; \TA) = p(\vf^n; \TA)\\
%\pB(\vx_2^n; \TB) = p(\ve^n; \TB)
%\end{align*}
%
%In either case, independently optimizing each model entails the maximization of its data log-likelihood (for $k\in{1,2}$):
%$L_k(\set{\vx^n_k}; \Tk)=\sum_n \log\pk(\vx_k^n; \Tk)$
%%
%%\begin{align*}
%%\max_{\Tk}\,\,L_k(\set{\vx^n_k}; \Tk)= 
%%\max_{\Tk}\,\,  \sum_n \log \pk(\vx_k^n; \Tk)
%%\end{align*}
%
%To jointly optimize the two models, a coupling term $R(\TA, \TB)$ is introduced, which encodes a measure of agreement between the two models.
%The resulting joint optimization problem takes the following form:
%\begin{equation}
%\max_{\TA, \TB}\,\, \lambda R(\TA, \TB)
%%+\sum_{k\in\set{1,2}} \log \sum_n \pk(\vx^n; \Tk)
%+\sum_{k\in\set{1,2}} L_k(\set{\vx_k^n}; \Tk)
%\label{eqn:joint}
%\end{equation}
%with $\lambda\ge0$ being a tunable hyperparameter.
%
%\subsection{Parameter Agreement Measure}
%Both the word alignment and transliteration models presented in Section \ref{sec:background} are parameterized by a translation table.
%Considering these tables in both directions, $t_1$ and $t_2$, we suggest the following parameter agreement measure: 
%\begin{equation}
%R(\TA, \TB) = \sum_{e,f} \sqrt{t_1(f \mid e) \cdot t_2(e \mid f)}
%\label{eqn:R}
%\end{equation}
%where $\ve$ and $\vf$ range over all source and target entity types.
%Note that $R$ disregards the distortion parameters $a$ (if at all present).
%
%Our agreement measure has several appealing properties.
%Using the Cauchy-Schwarz inequality, we can bound $R(\TA, \TB) \le \sqrt{|V_E|\cdot|V_F|}$ where $|V_E|, |V_F|$ denote the source and target vocabulary size.
%%\begin{align*}
%%R(\TA, \TB) 
%%%\sum_{e,f} \sqrt{(t_1(\ve \mid \vf) \cdot t_2(\vf \mid \ve))} 
%%\le &\sqrt{ (\sum_{e,f} t_1(\vf \mid \ve)) (\sum_{e,f} t_2(\ve \mid \vf))}\\
%%= &\sqrt{(|V_E|\cdot|V_F|)}
%%\end{align*}
%In particular, when $|V_E|=|V_F|$, the maximum is attained on parameter configurations for which $t_1(\vf \mid \ve) = t_2(\ve \mid \vf)$ over all entity pairs.
%% demonstrating that this agreement term is not biased towards any particular property (e.g, sparsity).
%Thus, the objective in \eqn{eqn:joint} balances between similarity in parameter magnitude and data likelihood. 
%%\marginpar{\bluetext{TL: this seems like a good place to say something about invertibility}}
%
%Another appealing property of $R$ is its concavity, which follows since $R$ is the sum of concave functions $h(x, y)=\sqrt{xy}$ over a closed convex set (see the concavity proof for $h$ in the appendix).
%
%% TODO: other agreement terms
%
%%We now describe our method, which we call Parameter Agreement Training.
%%In translation, we believe that if a word $e$ can be translated
%%to a word $f$, then $f$ can be translated to~$e$. 
%%This intuition holds for other tasks such as word or phoneme alignment.
%%Thus, a desirable form of model agreement is parameter
%%sparsity agreement: 
%%\begin{align*}
%%P_{0}(f\mid e) = 0 \text{ iff } P_{1}(e\mid f)=0.
%%\end{align*}
%%%A possibly simpler form of agreement is $P_{0}(e\,,f) = P_{1}(f,\: e)$ which encodes
%%% equality in joint distributions.
%
%%\subsection{Regularization}
%%To encourage this form of parameter agreement we add a regularizer that couples the two models together:
%%\begin{align}
%%\max_{\PP\,,\QQ}&\,\,L(\PP | X)+L(\QQ | X)+\lambda R(\PP,\, \QQ)
%%\end{align}
%%where $\lambda\ge0$ is a regularization coefficient to be tuned and
%%\begin{align}
%%R(\PP, \QQ) = \sum_{e,f}\sqrt{\PP(f\mid e)\QQ(e\mid f)} .
%%\end{align}
%%This regularizer has two attractive properties:
%%\begin{itemize}
%%\item It is concave on the probability simplex which lends to an efficiently solvable convex optimization program when the log-likelihood terms are themselves concave.
%
%%Concavity proof: 
%%The regularizer is a sum of concave functions defined over a convex set. 
%%each summand is concave  the region $[0,1]^{2}$ and the sum of concave functions is concave)
%%\item It encourages parameter sparsity agreement - If $\PP(e \mid f) = 0$ but $\QQ(f\mid e) > 0$, shifting weight away from $\QQ(f \mid e)$ could only increase the regularizer value.
%%\end{itemize}
%%In practice, we found the following two regularizers useful:`
%%\begin{align*}
%%R_{GM}(P_{0},\, P_{1}) & =  \sum_{e,f}\sqrt{P_{0}(f\mid e)\cdot P_{1}(e\mid f)}\\
%%%R_{CD}(P_{0},\, P_{1}) & =  -\frac{1}{2}\sum_{e,f}(P_{0}(e\mid f)-P_{1}(f\mid e))^{2}
%%\end{align*}
%%Where \emph{GM }stands for 'geometric mean' and \emph{CD} for 'conditional
%%difference'. 
%%
%%Note that both regularizers are concave on the probability simplex
%%- each term in their summation is concave over the region $[0,1]^{2}$,
%%and the sum of concave functions is concave. One difference between
%%$R_{GM}$ and $R_{CD}$ is that, while both regularizers attain higher
%%values when the conditional distributions agree, $R_{GM}$ attains
%%its maximum when those distributions are sparse (need example?)
%
%\subsection{Optimization Procedure}\label{subsec:optimization}
%Using a concave agreement measure, the optimization of \eqn{eqn:joint} neatly falls under the EM framework \cite{Dempster:1977}.
%
%In the E-step, each model $\Tk$, $k\in\set{1,2}$ is held fixed and its posterior distribution over the missing data $\vz_k^n$ is computed per each observation $\vx_k^n$:
%\begin{align*}
%q_k(\vz\kn, \vx\kn) & \mathrel{:=} \pk(\vz\kn \mid \vx\kn ; \Tk)
%%\textbf{parallel data}: q_k(\va \mid \ve, \vf) :=& \pA(\va \mid \ve, \vf; \TA) \\
%%\textbf{decipherment}: q_1(\va, \ve \mid \vf) :=& \pA(\va, \ve \mid \vf; \TA)
%\end{align*}
%where, in the parallel data setting, only the alignment is missing ($\vz\kn = \va\kn$) and in the non-parallel data setting, both the alignment and the origin entity are missing
%($\vz_1^n = (\va_1^n, \ve^n), \vz_2^n = (\va_2^n, \vf^n)$).
%
%Both the word alignment and transliteration models considered in this paper can be encoded as wFST instances. This implies that the E-step posteriors $q_k$ can be efficiently computed using dynamic programming \cite{Eisner02}. Specifically, for the alignment and transliteration models, this amounts to computing the expected counts  $\E_1[C(e,f)] $ and $\E_2[C(e,f)] $, where $C(e,f)$ is the number of times $e$ is seen aligned to $f$ in a alignment sequence. 
%
%In the M-step, the computed posteriors are used to construct the coupled sum of expected complete data log-likelihoods. The resulting expression is maximized with respect to the model parameters:
% %$\TA, \TB$:
%\begin{align*}
%(\TA', \TB') & \mathrel{:=} \arg\max_{\TA, \TB}
%\lambda R(\TA,\TB)\\
%&\quad {} + \sum_{k, n} q_k(\vz\kn, \vx\kn) \log p_k(\vx\kn, \vz\kn)
%\end{align*}
%with $k\in\set{1,2}$ and $n$ ranging over the appropriate set of samples. For the IBM and the HMM word alignment models, the modeling assumptions allow the distortion and the translation parameters to be optimized independently in the M-step. Applying the parameter agreement measure to the translation probabilities (Equation~\ref{eqn:R}), the above objective function can be defined for $t_1(f \mid e)$ and $t_2(e \mid f)$ as:
%\begin{align*}
%&\sum_{e,f}  \E_1[C(e,f)] \log t_1(f \mid e)  + {}\\ 
%&\quad \E_2[C(e,f)] \log t_2(e \mid f) + \sqrt{t_1(f \mid e)t_2(e \mid f)}
%\end{align*}
%
%\noindent
%where the expected counts were computed in the E-step. This objective function applies for the $t_1(f \mid e)$ and $t_2(e \mid f)$ parameters in transliteration as well. Since we only encourage agreement between $t_1$ and $t_2$, the estimation of the distortion probabilities is left unchanged. We leave agreement training of the distortion parameters for future work. 
%
%Owing to the concavity of both $R$ and the data log-likelihood functions, this maximization problem can be efficiently solved using convex programming techniques. 
%We use projected gradient descent \cite{bertsekas1999nonlinear} where at each step, the current $(\TA, \TB)$ parameters are updated according to the gradient of the M-step objective and then projected back onto the probability simplex. We used simple stopping conditions based on objective function convergence or a limited number of iterations.
%
%\iffalse
%\subsection{PAT for Word Alignment}
%As described in section~\ref{subsec:optimization}, in the E-step, we compute the posterior distribution over alignments in each direction, which amounts to computing the expected counts  $\E_1[C(e,f)] $ and $\E_2[C(e,f)] $, where $C(e,f)$ is the number of times $e$ is seen aligned to $f$. 
%In the M-step, we use the expected counts to re-estimate model parameters by maximizing the coupled expected complete data log likelihood. 
%In our work, we only encourage agreement between the translation probabilities $t(e \mid f)$ and $t(f \mid e)$. 
%Thus, the M-step objective function with respect to the translation probabilities is:
%\begin{align*}
%&\sum_{e,f}  \E_1[C(e,f)] \log t(e \mid f)  + {}\\ 
%&\quad \E_2[C(e,f)] \log t(f \mid e) + \sqrt{t(e \mid f)t(f \mid e)}, 
%\end{align*}
%which is maximized with projected gradient descent. 
%
%The estimation of the distortion probabilities is thus left unchanged (count and divide). We leave PAT of the distortion parameters for future work. 
%\fi
%
%
%%At each time step, the models $\PP, \QQ$ are updated according to the gradient of the regularized objective function 
%%%$\frac{\partial}{\partial \PP}(L(\PP \mid X)+\lambda R(\PP, \QQ)$
%%and projected back onto the probability simplex. 
%%
%%Inspecting the regularizer's gradient we see that each update brings the two models closer:
%%\begin{align*}
%%\frac{\partial R}{\partial t_1(\vf\mid\ve)}
%%&=\sqrt{\frac{t_2(\ve\mid\vf)}{2 t_1(\vf\mid\ve)}} \\
%%\frac{\partial R}{\partial t_2(\ve\mid\vf)}
%%&=\sqrt{\frac{t_1(\vf\mid\ve)}{2 t_2(\ve\mid\vf)}}
%%\end{align*}
%%%
%%With symmetric terms for the gradient in $\QQ$. Note that the square root is taken entry-wise.