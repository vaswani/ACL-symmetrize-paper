\subsection{Agreement Training}
In word alignment, it is common practice to train alignment models in both source-to-target and target-to-source directions and then apply symmetrization techniques to the resulting source-to-target and target-to-source alignments. 
%Alignment symmetrization heuristics, such as intersection, union or grow-diag-final-and play a significant role in reducing alignment errors and ultimately improving translation quality.
The improvement gained by these post-processing heuristics suggest that each (directional) model suffers from different errors, and that those errors can be mitigated by symmetrization.

Observing this, \newcite{liang+:2006:align} formulate Alignment by Agreement, which maximizes both the observed data-likelihoods as well as a measure of agreement between the two models' posteriors.
Showing significant reductions in AER, they demonstrate the effectiveness of jointly learning the models. However, because both symmetrization heuristics and Alignment by Agreement try to create agreement between the inferences of the two models, they necessarily run the two models on the same data, which must be parallel.

In this section, we describe Parameter Agreement Training (PAT), a method for jointly learning directional models that does not depend on parallel data. 	
We follow Liang et al. (2006) in formulating a joint objective function for training the two directional alignment models, but define the agreement measure in a way that can be applied either to the parallel data or non-parallel data setting.

In the parallel data setting, we observe $N$ paired sequences $\set{\vx^n_1}_{n=1}^N=\set{(\ve^n, \vf^n)}_{n=1}^N$ or, equivalently, $\set{\vx^n_2}_{n=1}^N=\set{(\vf^n, \ve^n)}_{n=1}^N$.

In the non-parallel data setting, we observe two sets of data points $\set{\vx^n_1}_{n=1}^{N_1} = \set{\ve^n}_{n=1}^{N_1}$ and $\set{\vx^n_2}_{n=2}^{N_2} = \set{\vf^n}_{n=1}^{N_2}$.

Let $\TA$ and $\TB$ represent the two directional models' parameters.
For $k\in\set{1,2}$, the probability of the $n$th sample under the $k$th model is denoted $\pk(\vx_k^n; \Tk)$.
Specifically, in the parallel data setting, the directional probability of $\vx_k^n$ under its model is:
\begin{align*}
\pA(\vx_1^n; \TA) = p(\vf^n \mid \ve^n; \TA)\\
\pB(\vx_2^n; \TB) = p(\ve^n \mid \vf^n; \TB)
\end{align*}
whereas in the non-parallel data setting, the probability is defined:
\begin{align*}
\pA(\vx_1^n; \TA) = p(\vf^n; \TA)\\
\pB(\vx_2^n; \TB) = p(\ve^n; \TB)
\end{align*}

In either case, independently optimizing each model entails the maximization of its data log-likelihood (for $k\in{1,2}$):
$L_k(\set{\vx^n_k}; \Tk)=\sum_n \log\pk(\vx_k^n; \Tk)$
%
%\begin{align*}
%\max_{\Tk}\,\,L_k(\set{\vx^n_k}; \Tk)= 
%\max_{\Tk}\,\,  \sum_n \log \pk(\vx_k^n; \Tk)
%\end{align*}

To jointly optimize the two models, a coupling term $R(\TA, \TB)$ is introduced, which encodes a measure of agreement between the two models.
The resulting joint optimization problem takes the following form:
\begin{equation}
\max_{\TA, \TB}\,\, \lambda R(\TA, \TB)
%+\sum_{k\in\set{1,2}} \log \sum_n \pk(\vx^n; \Tk)
+\sum_{k\in\set{1,2}} L_k(\set{\vx_k^n}; \Tk)
\label{eqn:joint}
\end{equation}
with $\lambda\ge0$ being a tunable hyperparameter.

\subsection{Parameter Agreement Measure}
Both the word alignment and transliteration models presented in Section \ref{sec:background} are parameterized by a translation table.
Considering these tables in both directions, $t_1$ and $t_2$, we suggest the following parameter agreement measure: 
\begin{equation}
R(\TA, \TB) = \sum_{e,f} \sqrt{t_1(f \mid e) \cdot t_2(e \mid f)}
\label{eqn:R}
\end{equation}
where $\ve$ and $\vf$ range over all source and target entity types.
Note that $R$ disregards the distortion parameters $a$ (if at all present).

Our agreement measure has several appealing properties.
Using the Cauchy-Schwarz inequality, we can bound $R(\TA, \TB) \le \sqrt{|V_E|\cdot|V_F|}$ where $|V_E|, |V_F|$ denote the source and target vocabulary size.
%\begin{align*}
%R(\TA, \TB) 
%%\sum_{e,f} \sqrt{(t_1(\ve \mid \vf) \cdot t_2(\vf \mid \ve))} 
%\le &\sqrt{ (\sum_{e,f} t_1(\vf \mid \ve)) (\sum_{e,f} t_2(\ve \mid \vf))}\\
%= &\sqrt{(|V_E|\cdot|V_F|)}
%\end{align*}
In particular, when $|V_E|=|V_F|$, the maximum is attained on parameter configurations for which $t_1(\vf \mid \ve) = t_2(\ve \mid \vf)$ over all entity pairs.
% demonstrating that this agreement term is not biased towards any particular property (e.g, sparsity).
Thus, the objective in \eqn{eqn:joint} balances between similarity in parameter magnitude and data likelihood. 
%\marginpar{\bluetext{TL: this seems like a good place to say something about invertibility}}

Another appealing property of $R$ is its concavity, which follows since $R$ is the sum of concave functions $h(x, y)=\sqrt{xy}$ over a closed convex set (see the concavity proof for $h$ in the appendix).

% TODO: other agreement terms

%We now describe our method, which we call Parameter Agreement Training.
%In translation, we believe that if a word $e$ can be translated
%to a word $f$, then $f$ can be translated to~$e$. 
%This intuition holds for other tasks such as word or phoneme alignment.
%Thus, a desirable form of model agreement is parameter
%sparsity agreement: 
%\begin{align*}
%P_{0}(f\mid e) = 0 \text{ iff } P_{1}(e\mid f)=0.
%\end{align*}
%%A possibly simpler form of agreement is $P_{0}(e\,,f) = P_{1}(f,\: e)$ which encodes
%% equality in joint distributions.

%\subsection{Regularization}
%To encourage this form of parameter agreement we add a regularizer that couples the two models together:
%\begin{align}
%\max_{\PP\,,\QQ}&\,\,L(\PP | X)+L(\QQ | X)+\lambda R(\PP,\, \QQ)
%\end{align}
%where $\lambda\ge0$ is a regularization coefficient to be tuned and
%\begin{align}
%R(\PP, \QQ) = \sum_{e,f}\sqrt{\PP(f\mid e)\QQ(e\mid f)} .
%\end{align}
%This regularizer has two attractive properties:
%\begin{itemize}
%\item It is concave on the probability simplex which lends to an efficiently solvable convex optimization program when the log-likelihood terms are themselves concave.

%Concavity proof: 
%The regularizer is a sum of concave functions defined over a convex set. 
%each summand is concave  the region $[0,1]^{2}$ and the sum of concave functions is concave)
%\item It encourages parameter sparsity agreement - If $\PP(e \mid f) = 0$ but $\QQ(f\mid e) > 0$, shifting weight away from $\QQ(f \mid e)$ could only increase the regularizer value.
%\end{itemize}
%In practice, we found the following two regularizers useful:`
%\begin{align*}
%R_{GM}(P_{0},\, P_{1}) & =  \sum_{e,f}\sqrt{P_{0}(f\mid e)\cdot P_{1}(e\mid f)}\\
%%R_{CD}(P_{0},\, P_{1}) & =  -\frac{1}{2}\sum_{e,f}(P_{0}(e\mid f)-P_{1}(f\mid e))^{2}
%\end{align*}
%Where \emph{GM }stands for 'geometric mean' and \emph{CD} for 'conditional
%difference'. 
%
%Note that both regularizers are concave on the probability simplex
%- each term in their summation is concave over the region $[0,1]^{2}$,
%and the sum of concave functions is concave. One difference between
%$R_{GM}$ and $R_{CD}$ is that, while both regularizers attain higher
%values when the conditional distributions agree, $R_{GM}$ attains
%its maximum when those distributions are sparse (need example?)

\subsection{Optimization Procedure}\label{subsec:optimization}
Using a concave agreement measure, the optimization of \eqn{eqn:joint} neatly falls under the EM framework.

In the E-step, each model $\Tk$, $k\in\set{1,2}$ is held fixed and its posterior distribution over the missing data $\vz_k^n$ is computed per each observation $\vx_k^n$:
\begin{align*}
q_k(\vz\kn, \vx\kn) & \mathrel{:=} \pk(\vz\kn \mid \vx\kn ; \Tk)
%\textbf{parallel data}: q_k(\va \mid \ve, \vf) :=& \pA(\va \mid \ve, \vf; \TA) \\
%\textbf{decipherment}: q_1(\va, \ve \mid \vf) :=& \pA(\va, \ve \mid \vf; \TA)
\end{align*}
where, in the parallel data setting, only the alignment is missing ($\vz\kn = \va\kn$) and in the non-parallel data setting, both the alignment and the origin entity are missing
($\vz_1^n = (\va_1^n, \ve^n), \vz_2^n = (\va_2^n, \vf^n)$).

Both the word alignment and transliteration models considered in this paper can be encoded as FST instances. This implies that the E-step posteriors $q_k$ can be efficiently computed using dynamic programming \cite{Eisner02}. Specifically, for the alignment and transliteration models, this amounts to computing the expected counts  $\E_1[C(e,f)] $ and $\E_2[C(e,f)] $, where $C(e,f)$ is the number of times $e$ is seen aligned to $f$ in a alignment sequence. 

In the M-step, the computed posteriors are used to construct the coupled sum of expected complete data log-likelihoods. The resulting expression is maximized with respect to the model parameters:
 %$\TA, \TB$:
\begin{align*}
(\TA', \TB') & \mathrel{:=} \arg\max_{\TA, \TB}
\lambda R(\TA,\TB)\\
&\quad {} + \sum_{k, n} q_k(\vz\kn, \vx\kn) \log p_k(\vx\kn, \vz\kn)
\end{align*}
with $k\in\set{1,2}$ and $n$ ranging over the appropriate set of samples. For the IBM and the HMM word alignment models, the modeling assumptions allow the distortion and the translation parameters to be optimized independently in the M-step. Applying the parameter agreement measure to the translation probabilities (Equation~\ref{eqn:R}), the above objective function can be defined for $t_1(f \mid e)$ and $t_2(e \mid f)$ as:
\begin{align*}
&\sum_{e,f}  \E_1[C(e,f)] \log t_1(f \mid e)  + {}\\ 
&\quad \E_2[C(e,f)] \log t_2(e \mid f) + \sqrt{t_1(f \mid e)t_2(e \mid f)}
\end{align*}

\noindent
where the expected counts were computed in the E-step. This objective function applies for the $t_1(f \mid e)$ and $t_2(e \mid f)$ parameters in transliteration as well. Since we only encourage agreement between $t_1$ and $t_2$, the estimation of the distortion probabilities is left unchanged. We leave agreement training of the distortion parameters for future work. 

Owing to the concavity of both $R$ and the data log-likelihood functions, this maximization problem can be efficiently solved using convex programming techniques. 
We used our own projected gradient descent implementation (PGD). At each step, the current $(\TA, \TB)$ parameters are updated according to the gradient of the M-step objective and then projected back onto the probability simplex. We used simple stopping conditions based on objective function convergence or a limited number of iterations.

\iffalse
\subsection{PAT for Word Alignment}
As described in section~\ref{subsec:optimization}, in the E-step, we compute the posterior distribution over alignments in each direction, which amounts to computing the expected counts  $\E_1[C(e,f)] $ and $\E_2[C(e,f)] $, where $C(e,f)$ is the number of times $e$ is seen aligned to $f$. 
In the M-step, we use the expected counts to re-estimate model parameters by maximizing the coupled expected complete data log likelihood. 
In our work, we only encourage agreement between the translation probabilities $t(e \mid f)$ and $t(f \mid e)$. 
Thus, the M-step objective function with respect to the translation probabilities is:
\begin{align*}
&\sum_{e,f}  \E_1[C(e,f)] \log t(e \mid f)  + {}\\ 
&\quad \E_2[C(e,f)] \log t(f \mid e) + \sqrt{t(e \mid f)t(f \mid e)}, 
\end{align*}
which is maximized with projected gradient descent. 

The estimation of the distortion probabilities is thus left unchanged (count and divide). We leave PAT of the distortion parameters for future work. 
\fi


%At each time step, the models $\PP, \QQ$ are updated according to the gradient of the regularized objective function 
%%$\frac{\partial}{\partial \PP}(L(\PP \mid X)+\lambda R(\PP, \QQ)$
%and projected back onto the probability simplex. 
%
%Inspecting the regularizer's gradient we see that each update brings the two models closer:
%\begin{align*}
%\frac{\partial R}{\partial t_1(\vf\mid\ve)}
%&=\sqrt{\frac{t_2(\ve\mid\vf)}{2 t_1(\vf\mid\ve)}} \\
%\frac{\partial R}{\partial t_2(\ve\mid\vf)}
%&=\sqrt{\frac{t_1(\vf\mid\ve)}{2 t_2(\ve\mid\vf)}}
%\end{align*}
%%
%With symmetric terms for the gradient in $\QQ$. Note that the square root is taken entry-wise.