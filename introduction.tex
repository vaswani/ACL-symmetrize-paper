Word alignment is an important part of any statistical translation pipeline. The most widely used word alignment models are the the IBM ~\shortcite{brown+alii:1993} and HMM ~\shortcite{vogel+alii:1996} models. These models describe a generative process by which the source sentence generates ($E$) the target sentence ($F$) via one-to-many alignments. The models are trained independently in each direction, and at test time, the alignments from both directions are \emph{symmetrized} using heuristics like \emph{grow-diag-final} ~\shortcite{och+ney:2003}. Although symmetrization corrects some of the mistakes that the independently trained models make, ~\newcite{liang+:2006:align} and ~\newcite{ganchev2010posterior} show that word alignment quality be improved when the models are trained jointly to agree on their inferences. However, in ~\newcite{liang+:2006:align}, the procedure relies on parallel data, which might not always be the available, for example, in unsupervised transliteration with non-parallel data \marginpar{cite sujith and knight}. In ~\newcite{ganchev2010posterior}, the training procedure can be expensive, making it difficult to scale to large data. In this paper, we present an approach to improve unsupervised transliteration quality (section~\ref{sec:transliteration}) over a state-of-the-art baseline \marginpar{cite sujith} by jointly training transliteration models in $English-Japanese$ and $Japanese-English$ directions that does not rely on parallel data. Unlike ~\shortcite{liang+:2006:align} and ~\shortcite{ganchev2010posterior}, our approach encourages agreement in the parameters of the models. We also apply our procedure on joint training of word alignment models and we achieve results comparable to ~\newcite{liang+:2006:align} on multiplie language pairs. 

\iffalse
predictions by symmetrization increases the overall predictive performance.
Several papers address this issue directly by jointly training these
asymmetric models with a particular notion of agreement in mind {[}reference{]}.
Generally speaking, these techniques encourage the two models to agree
on their inferences (posteriors) either on each sentence pair individually,
or over all sentence pairs together.

{[}{[}However, agreement on inference essentially treats the two models
themselves as black-boxes{]}{]}

In this paper, we explore a different type of model agreement we call
parameter agreement... (needs more content)

\fi