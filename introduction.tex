\section{Introduction}
In word alignment it is has long been observed that (independently)
training alignment models in both directions and then combining their
predictions by symmetrization increases the overall predictive performance.
Several papers address this issue directly by jointly training these
asymmetric models with a particular notion of agreement in mind {[}reference{]}.
Generally speaking, these techniques encourage the two models to agree
on their inferences (posteriors) either on each sentence pair individually,
or over all sentence pairs together.

{[}{[}However, agreement on inference essentially treats the two models
themselves as black-boxes{]}{]}

In this paper, we explore a different type of model agreement we call
parameter agreement... (needs more content)