\iffalse
Many tasks in machine translation (MT) are first approached by modeling the transfer of information from one language to another (say, English to French). 
The resulting directional models often follow a simple generative story that explains how source language entities could have generate corresponding target language entities. 
For example, the IBM word alignment models \cite{Brown1993} explain how strings composing an English sentence can be translated and rearranged (distorted) into strings composing a French sentence and pronunciation Finite State Transducer (FST) can be used to explain how names in English undergo phonetic changes as they are uttered in French.

However, the directionality may come with a price, as it impose restriction on the inference space - in the word alignment models (IBM 1 or 2 and HMM), only one-to-many word alignments (target to source) are possible, which may not fit well for some language pairs (for example, German compound target words usually arise from multiple English source words). 
For this reason, it is common practice to produce word alignment inferences using models trained in both directions and then apply a post-processing symmetrization technique (such as grow-diag-final-and).
However, while post-processing resolves some of the problems, independent training can still lead the two models to divergent local maxima; ones whose disagreeing inferences cannot be fixed by post-processing techniques.

In Alignment by Agreement (ABA), Liang et al. (2006) suggest to jointly train the two directional models, each maximizing its own data likelihood (as would be done independently) as well as a model coupling term that encodes some measure of agreement between the models.
Their suggested agreement measure further exploits the observed parallel training data by rewarding similarity between the two alignment posteriors (one for each direction) computed on each training sentence pair.
Indeed, their experimental results show that training with posterior agreement leads to better performing models (reduced AER). At the same time however, the ABA formulation cements the dependency on parallel data thus limiting the applicability of their method to other settings.

In this paper, we present Parameter Agreement Training (PAT); a method that operates with a different measure of agreement in mind (Section \ref{sec:model}))
Remaining in context of word alignment (as we have so far), the general idea is as follows: Consider the two directional word alignment models' translation tables. 
For any bilingual pair of words $e$ and $f$, it seems reasonable that $t(f \mid e)$ is high/low whenever $t(e \mid f)$ is high/low.
This intuition follows simply since word translation is generally invertible \footnote{The word $e$ should be in the set of translations of the translation of $e$}.
With the above in mind, we designed an agreement measure that encourages agreement in the two directional models parameters magnitudes.
Our resulting objective function can be optimized using EM, where the E-step remains unchanged and the M-step reduces to an instance of convex programming.

Furthermore, our new agreement measure is free of the parallel data dependency which in turn broadens the range of settings our method can be applied to beyond conventional word alignment.
In particular, our method can be applied in the challenging decipherment setting, under which no parallel data is provided.

We conduct two sets of experiments (Section \ref{bla}): (1) Our word alignment experiments (with parallel data) show comparable F- and BLEU score improvements to those of ABA on various language pairs.
Combining the phrase-tables of both ABA and PAT leads to further increase in BLEU score.
(2) In the decipherment settings (without parallel data) we repeat the Japanese to English back-transliteration experiments as described by Ravi and Knight (2009?). 
PAT nearly splits the difference between the baseline (Ravi and Knight's formulation) and a method trained using parallel data. 
Finally, ... ?
\fi

Alignment is a crucial subtask for many larger NLP problems: Word alignments are the backbone for extracting translation rules used in statistical machine translation; phoneme alignments are used to transliterate words in one language to another. The goal is to align entities, which could be words or phonemes, between two sequences, which could be pairs of source and target sentences, for example French and English sentences, or pairs of phoneme sequences, for example English phoneme and Japanese katakana sequences. 

The popular models used in these tasks are both \emph{generative} and \emph{directed}, that is, they define a directed sequence of events by which one side of the pair produces the other side. For example, the IBM models for word alignment \newcite{brown+alii:1993} describe a stochastic process by which the target sentence is generated from the source sentence via word alignments. These processes can sometimes produce different alignments, depending on the direction of generation. However, the directionality may come with a price, as it impose restriction on the inference space - in the word alignment models (IBM 1 or 2 and HMM), only one-to-many word alignments (target to source) are possible, which may not fit well for some language pairs (for example, German compound target words usually arise from multiple English source words). To remedy this, we resort to ad-hoc alignment symmetrization approaches , such as grow-diag-final-and \cite{koehn+:2003}, \emph{after} training. However, while post-processing resolves some of the problems, independent training can still lead the two models to divergent local maxima; ones whose disagreeing inferences cannot be fixed by post-processing techniques. 

A second problem for training alignment models is the need for parallel data. For example, in \newcite{KG98}, the authors use (some number) \marginpar{insert number} of parallel English-Katana sequences to training phoneme alignment models. While large amounts of parallel data needed for word alignment is readily available for many language pairs, acquiring parallel phoneme sequences for transliteration can be very challenging. 

There has been previous work on solving the problem of model disagreement \cite{liang+:2006:align} (ABA), and training phoneme alignment models with monolingual data \cite{RK09}. \newcite{liang+:2006:align} suggest an approach, Alignment by Agreement (ABA), to jointly train the two directional models, each maximizing its own data likelihood (as would be done independently) as well as a model coupling term that encodes some measure of agreement between the models. Their suggested agreement measure further exploits the observed parallel training data by rewarding similarity between the two alignment posteriors (one for each direction) computed on each training sentence pair. Indeed, their experimental results show that training with posterior agreement leads to better performing models (reduced AER). However, in \newcite{liang+:2006:align}, the authors do not optimize a clear objective function and their approach cannot be applied to training alignment models without parallel data. For phoneme alignment, \newcite{RK09} alleviate the reliance on parallel data and improve monolingual alignment quality over the baseline, but their approach does not encourage model agreement, which can further improve alignment accuracy, as we will show in section~\ref{sec:results}. 

In our paper, we present a \emph{single} approach that achieves both model agreement during training, and can be used in non-parallel settings. Our approach, Parameter Agreement Training (PAT), operates with a different measure of agreement in mind (Section \ref{sec:model})). The general idea can be easily illustrated in the word alignment setting: Consider the two directional word alignment models' translation tables.  For any pair of words $e$ and $f$, it seems reasonable that $t(f \mid e)$ is high(low) whenever $t(e \mid f)$ is high(low). This intuition follows simply since word translation is generally invertible \footnote{The word $e$ should be in the set of translations of the translation of $e$}.
With the above in mind, we design an agreement measure that encourages agreement in the magnitudes of the parameters of the models in each direction.
Our resulting objective function can be optimized using EM, where the E-step remains unchanged and the M-step reduces to an instance of convex programming. Furthermore, since our approach encourages agreement between the model parameters, it can be applied in non-parallel settings such as unsupervised phoneme alignment with monolingual data. Using our approach, we are able to significantly improve over \newcite{RK09}, achieving a $50\%$ reduction in error. For word alignment, our results compare favorably with results from previous approaches for model agreement. 

%The generative process for word alignment allows a target word to align to \emph{at most} one source word, producing different alignments in the source-target vs target-source directions.


 
%%%% old introduction
%Word alignment is an important part of any statistical translation pipeline. The most widely used word alignment models are the the IBM \cite{brown+alii:1993} and HMM \cite{vogel+alii:1996} models. These models describe a generative process by which the source sentence ($E$) generates the target sentence ($F$) via one-to-many alignments. The models are trained independently in each direction, and at test time, the alignments from both directions are \emph{symmetrized} using heuristics like \emph{grow-diag-final} \cite{och+ney:2003}. Although symmetrization corrects some of the mistakes that the independently trained models make, \newcite{liang+:2006:align} and \newcite{ganchev2010posterior} show that word alignment quality be improved when the models are trained jointly to agree on their inferences. However, the method of \newcite{liang+:2006:align} relies on parallel data, which might not always be the available, for example, in unsupervised transliteration with non-parallel data \cite{RK09}. Another method, posterior regularization \cite{ganchev2010posterior}, can be expensive, making it difficult to scale to large data. In this paper, we present an approach that improves unsupervised transliteration quality (section~\ref{sec:transliteration}) over a state-of-the-art baseline by jointly training English-Japanese and Japanese-English transliteration models on non-parallel data. Unlike the methods of \newcite{liang+:2006:align} and \newcite{ganchev2010posterior}, our approach encourages agreement in the parameters of the models (sections \ref{sec:background} and \ref{sec:method}). We also apply our procedure on joint training of word alignment models (section ~\ref{sec:alignment}) and we achieve results comparable to those of \newcite{liang+:2006:align} on multiple language pairs. 
%
%\iffalse
%predictions by symmetrization increases the overall predictive performance.
%Several papers address this issue directly by jointly training these
%asymmetric models with a particular notion of agreement in mind {[}reference{]}.
%Generally speaking, these techniques encourage the two models to agree
%on their inferences (posteriors) either on each sentence pair individually,
%or over all sentence pairs together.
%
%{[}{[}However, agreement on inference essentially treats the two models
%themselves as black-boxes{]}{]}
%
%In this paper, we explore a different type of model agreement we call
%parameter agreement... (needs more content)
%
%\fi