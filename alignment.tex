Given a \emph{target} French sentence, $\mathbf{f_{1}^{J}} =  f_{1}, f_{2}, \ldots , f_{j}, \ldots, f_{J}$, and a \emph{source} English sentence, $\mathbf{e_{1}^{I}} =  e_{1}, e_{2}, \ldots , e_{i}, \ldots, e_{I}$, word alignment models describe the generative process by which the source sentence creates the target ($E \rightarrow F$) ausing latent alignments $\mathbf{a_{1}^{J}} = a_{1}, a_{2}, \ldots , a_{j}, \ldots, a_{J}$. The alignment variable $a_{j}$ specifies the English word $e_{a_{j}}$ that the French word $f_j$ is aligned to. 

The IBM Models 1--2 and the HMM alignment models have two sets of parameters, the translation probabilities $P_{t}(f_{j} \mid e_{a_{j}})$ and distortion probabilities, $P_{d}(a_{j}\mid a_{j-1},j)$. These models differ in their implementation and estimation of the distortion probabilities, but share the same translation probabilities. The general form of the joint probability  of a target sentence and alignment given the source sentence is:

\begin{equation} \label{eq:joint-prob-align}
P(\mathbf{f_{1}^{J}}, \mathbf{a_{1}^{J}} \mid \mathbf{e_{1}^{I}})  = \prod_{j=1}^{J} P_{d}(a_{j}\mid a_{j-1},j) P_{t}(f_{j} \mid e_{a_{j}})
\end{equation}

During training, we typically estimate the parameters that maximize 
\begin{equation}
\log \sum_{a_{1}^{J}}  P(\mathbf{f_{1}^{J}}, \mathbf{a_{1}^{J}} \mid \mathbf{e_{1}^{I}}) = \log P(\mathbf{f_{1}^{J}} \mid \mathbf{e_{1}^{I}})
\end{equation}.

For details on parameter estimation of these models, and how to deal with empty words, the reader can refer to ~\newcite{och+ney:2003}. For translation, we use the \emph{Viterbi} alignments, 
\begin{equation}
\hat{\mathbf{a_{1}^{J}}} = \argmax_{\mathbf{a_{1}^{J}}} P(\mathbf{f_{1}^{J}}, \mathbf{a_{1}^{J}} \mid \mathbf{e_{1}^{I}})
\end{equation}
The generative story allows each target word $f_{j}$ to align to only one source word $e_{a_j}$. This can be problematic when the target language has compound words that must align to two or more source language words. The standard solution is to train another model in the \emph{reverse} direction ($F \rightarrow E$), $P(\mathbf{e_{1}^{L}}, \mathbf{a_{1}^{L}} \mid \mathbf{f_{1}^{J}})$ and then symmetrize the Viterbi alignments in both directions using heuristics like \emph{grow-diag-final}  ~\cite{koehn+:2003}. Symmetrization remedies some of the mistakes that the independently trained models make, garbage collection in particular ~\cite{liang+:2006:align}. However, the $E \rightarrow F$ and  $F \rightarrow E$ models do not communicate during training, which could guide the parameters in the wrong direction. In ~\newcite{liang+:2006:align} and ~\newcite{ganchev2010posterior}, the authors show that training the alignment models jointly can improve alignment quality. In ~\newcite{liang+:2006:align}, the authors encourage the probabilities over alignments in each direction to agree, per sentence pair. However, this renders the inference intractable and the authors have to resort to an approximation, without specifying the objective that the approximate procedure ends up optimizing. In contrast, we optimize a clear objective which improves in every iteration. In ~\newcite{ganchev2010posterior}, the authors optimize a clear objective which encourages agreement between the alignments. However, their optimization procedure is expensive and scaling to large datasets is a challenge. \marginpar{need to make sure that it doesn't scale. I suspect it doesn't}.


