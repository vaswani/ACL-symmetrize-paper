%Given a \emph{target} French sentence, $\mathbf{f_{1}^{J}} =  f_{1}, f_{2}, \ldots , f_{j}, \ldots, f_{J}$, and a \emph{source} English sentence, $\mathbf{e_{1}^{I}} =  e_{1}, e_{2}, \ldots , e_{i}, \ldots, e_{I}$, word alignment models describe the generative process by which the source sentence creates the target ($E \rightarrow F$) ausing latent alignments $\mathbf{a_{1}^{J}} = a_{1}, a_{2}, \ldots , a_{j}, \ldots, a_{J}$. The alignment variable $a_{j}$ specifies the English word $e_{a_{j}}$ that the French word $f_j$ is aligned to. 
%
%The IBM Models 1--2 and the HMM alignment models have two sets of parameters, the translation probabilities $P_{t}(f_{j} \mid e_{a_{j}})$ and distortion probabilities, $P_{d}(a_{j}\mid a_{j-1},j)$. These models differ in their implementation and estimation of the distortion probabilities, but share the same translation probabilities. The general form of the joint probability  of a target sentence and alignment given the source sentence is:
%
%\begin{equation} \label{eq:joint-prob-align}
%P(\mathbf{f_{1}^{J}}, \mathbf{a_{1}^{J}} \mid \mathbf{e_{1}^{I}})  = \prod_{j=1}^{J} P_{d}(a_{j}\mid a_{j-1},j) P_{t}(f_{j} \mid e_{a_{j}})
%\end{equation}
%
%During training, we typically estimate the parameters that maximize 
%\begin{equation}
%\log \sum_{a_{1}^{J}}  P(\mathbf{f_{1}^{J}}, \mathbf{a_{1}^{J}} \mid \mathbf{e_{1}^{I}}) = \log P(\mathbf{f_{1}^{J}} \mid \mathbf{e_{1}^{I}})
%\end{equation}.
%
%For details on parameter estimation of these models, and how to deal with empty words, the reader can refer to ~\newcite{och+ney:2003}. For translation, we use the \emph{Viterbi} alignments, 
%\begin{equation}
%\hat{\mathbf{a_{1}^{J}}} = \argmax_{\mathbf{a_{1}^{J}}} P(\mathbf{f_{1}^{J}}, \mathbf{a_{1}^{J}} \mid \mathbf{e_{1}^{I}})
%\end{equation}
%The generative story allows each target word $f_{j}$ to align to only one source word $e_{a_j}$. This can be problematic when the target language has compound words that must align to two or more source language words. The standard solution is to train another model in the \emph{reverse} direction ($F \rightarrow E$), $P(\mathbf{e_{1}^{L}}, \mathbf{a_{1}^{L}} \mid \mathbf{f_{1}^{J}})$ and then symmetrize the Viterbi alignments in both directions using heuristics like \emph{grow-diag-final}  ~\cite{koehn+:2003}. Symmetrization remedies some of the mistakes that the independently trained models make, garbage collection in particular ~\cite{liang+:2006:align}. However, the $E \rightarrow F$ and  $F \rightarrow E$ models do not communicate during training, which could guide the parameters in the wrong direction. In ~\newcite{liang+:2006:align} and ~\newcite{ganchev2010posterior}, the authors show that training the alignment models jointly can improve alignment quality. In ~\newcite{liang+:2006:align}, the authors encourage the probabilities over alignments in each direction to agree, per sentence pair. However, this renders the inference intractable and the authors have to resort to an approximation, without specifying the objective that the approximate procedure ends up optimizing. In contrast, we optimize a clear objective which improves in every iteration. In ~\newcite{ganchev2010posterior}, the authors optimize a clear objective which encourages agreement between the alignments. However, their optimization procedure is expensive and scaling to large datasets is a challenge. \marginpar{need to make sure that it doesn't scale. I suspect it doesn't}.
%
%
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|c|c|c|c|}
%\cline{2-4} 
%\multicolumn{1}{c|}{} & de-en & cz-en & $\PP > 0.01$\tabularnewline
%\hline 
%~\newcite{liang+:2006:align}  & $71$\% & $70$\%  & -\tabularnewline
%\hline 
%PAT (our) & $71$\% & $69$\%  & -\tabularnewline
%\hline 
%\end{tabular}
%\label{tbl:Alignment Results}
%\caption{Alignment F-score improves.}
%\end{center}
%\end{table}

To test the effectiveness of parameter agreement training, we carried out Czech to English and Chinese to English translation experiments, comparing standard EM training, ABA training, and PAT. 

\subsection{Training}
Following standard practice, we first trained our word alignment models on the following parallel data:
\begin{itemize}
\item \textbf{Chinese-English}: selected data from the constrained task of the NIST 2009 Open MT Evaluation.
\item \textbf{Czech-English}: selected data from the Czech-English News Commentary corpus.
\end{itemize} 

For both ABA and vanilla EM training, we used the publicly available word alignment toolkits that implement them. GIZA++, the popular word alignment toolkit\footnote{https://code.google.com/p/giza-pp/}, implements EM training of the IBM and HMM alignment models, and the toolkit for for ABA \footnote{http://cs.stanford.edu/~pliang/software/cross-em-aligner-1.3.zip} trains IBM Model~1 and the HMM word alignment model. For both our baseline approaches, we ran two sets of experiments, one with $5$ iterations of Model~1 followed by  $5$ iterations of the HMM alignment model, and another with $10$ iterations of each. We trained word alignment models in both directions for both language pairs, and symmetrized our alignments with \emph{posterior decoding}, as described in \newcite{liang+:2006:align}. We will go over the details of posterior decoding in Section~\ref{sec:pos-decoding}.

\subsubsection{Parameter Agreement Training For Word Alignment}

As described in section~\ref{subsec:optimization}, in the E-step, we compute expected counts separately from the alignment models in each direction. Specifically, letting $\Theta_{1} = \Theta_{f \rightarrow e}$, and $\Theta_{2} = \Theta_{\mathbf{e} \rightarrow f}$, where $\rightarrow$ indicates the direction of generation in the word alignment generative story, we compute $E_{e \rightarrow f }[C(e,f)] $ and $E_{f \rightarrow e }[C(e,f)] $

\subsection{Posterior Decoding} \label{sec:pos-decoding}


\begin{table*}
\begin{center} \small
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{cc|c|l|l|l|lll}
task & data (M) & system & align F1 (\%)  &\multicolumn{3}{c}{\bleu (\%)} \\
     &      &        &             & 2008 & 2009 & 2010 \\
\hline
\multirow{3}{*}{Chi-Eng} & \multirow{3}{*}{5.3+6.6} & baseline & 64.6  &  23.6 & & \\
        &       & PAT &  70.0 (+5.4) & 24.0 (+0.4) & &  \\
        &       & ABA  & 70.8 (+6.2)& 24.4 (+0.8) & \\
        &       & ABA+PAT  &    & 25.1 (+1.5) & \\
\hline
\multirow{3}{*}{Cze-Eng} & \multirow{3}{*}{1.6+1.8} & baseline & 65.0 & & 16.7 & 17.1  \\
        &       & PAT   & 69.6 (+4.6)& & 17.1 (+0.4)& 17.6 (+0.5)& \\
        &       & ABA    & 70.4 (+5.4) & & 17.1 (+0.4)&  17.7  (+0.6) &\\
        &       & ABA+PAT   &  & &  17.4 (+0.7)& 17.9 (+0.8)&
\end{tabular}
\end{center}
\caption{For Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open MT Evaluation.}
\label{tab:results}
\end{table*}

