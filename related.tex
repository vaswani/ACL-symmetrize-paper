From a general machine learning perspective, our proposed method can be classified as an unsupervised regularized multi-task learning technique \cite{Evgeniou04,Caruana97}.

Our joint objective (\eqn{eqn:joint}) is inspired by ABA \cite{liang+:2006:align}, which suggests a joint optimization problem for training both directional models. 
The proposed agreement measure
$\sum_\vx \log \sum_{\vz} p_1(\vz \mid \vx; \TA) p_2(\vz \mid \vx; \TB)$
requires access to the posteriors of each sample ($\vx=(\ve, \vf)$) under \emph{both} models.
It is this dependency on parallel data that limits the applicability of the ABA formulation to the parallel data setting. 
Therefore, in some sense, our work can be viewed as a generalization of \newcite{liang+:2006:align} beyond the parallel data setting. \newcite{graca+alii:2010} also develop an approach that encourages agreement in bidirectional word alignment models. They achieve this by adding constraints on the posterior distributions over word alignments. However, their optimization requires many inference calls over the parallel data, making it unscalable. 

Current decipherment work seems to focus on large scale MT experiments \cite{Ravi11,Dou13}. 
We believe our approach would be beneficial in those settings as well.
%
%Bi-directional Conversion Between Graphemes and Phonemes Using a Joint N-gram Model. \bluetext{TL: This paper discusses a \textbf{joint} grapheme-to-phoneme model, trained on parallel data -- It may be out of scope.}
%





