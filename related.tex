%We describe two baseline approaches, contrast them with MIR and discuss other related work.

\subsection{Parallel Data Baseline: $\method{ABA}$}
Our approach is most similar to ``Alignment by Agreement'' \cite{liang+:2006:align} who first proposed a single joint objective for two word alignment models.
The difference between our objective (\eqn{eqn:joint}) and theirs lies in their proposed regularizer which rewards the per-sample agreement of the two models' alignment posteriors:
$$\sum_n \log \sum_\vz  p_1(\vz\mid \vx^n)\cdot p_2(\vz\mid \vx^n)$$
Where $\vx^n = (\ve^n, \vf^n)$ and where $\vz$ ranges over the possible alignments between $\ve^n$ and $\vf^n$ (Practically, only over 1-to-1 alignments, since each model is only capable of producing one-to-many alignments).

\newcite{liang+:2006:align} note that proper EM optimization of their regularized joint objective leads to an intractable E-step. 
%(computing the normalization constant per sample over the set of all possible alignments is a \#P-complete problem even for IBM model 1.)
Unable to exactly and efficiently compute alignment posteriors, they resort to a product-of-marginals heuristic which breaks EM's convergence guarantees, but has a closed-form solution and works well in practice.

Compared to $\method{ABA}$, $\method{MIR}$ regularization has both theoretical and practical advantages which make our method more convenient and broadly applicable:
\begin{enumerate}[leftmargin=1.2em]
\item By regularizing for posterior agreement, $\method{ABA}$ is restricted to a parallel data setting, whereas $\method{MIR}$ can be applied even without parallel data.
\item The posteriors of more advanced word alignment models (such as fertility-based models) do not correspond to alignments, and furthermore, are already estimated with approximation techniques.
Thus, even if we adapt $\method{ABA}$'s product-of-marginals heuristic to such models, we run the risk of estimating highly inaccurate posteriors (specifically, 0 valued posteriors).
In contrast, $\method{MIR}$ extends to all IBM-style word alignment models and does not add heuristics -- The M-step computation can be done exactly and efficiently with convex optimization. 

\item $\method{MIR}$ provides the same theoretical convergence guarantees as the underlying algorithms.
\end{enumerate}

We compare \method{MIR} against \method{ABA} in Section \ref{sec:alignment}.

\subsection{Non-Parallel Data Baseline: $\method{bi\mbox{-}EM}$ \label{sec:biEM}}
\newcite{Mylonakis2007} cast the two directional models as a single joint model by reparameterization and normalization.
That is, both directional models, consisting of a t-table only, are reparameterized as:
\begin{equation}
\tA(f\mid e) = \frac{\beta_{e,f}}{\sum_{f} \beta_{e,f}}
\,\,\text{ and }\,\,
\tB(e\mid f) = \frac{\beta_{e,f}}{\sum_{e} \beta_{e,f}}
\label{eqn:reparam}
\end{equation}
They then maximize the likelihood of observed \emph{monolingual} sequences from both languages:
\begin{equation}
\max_\beta L_1(\set{\vf^n}; \beta) + L_2(\set{\ve^n}; \beta)
\label{eqn:biEM}
\end{equation}
where for example:
\begin{align*}
L_1(\set{\vf^n}; \beta)
&=\log \prod_n p(\vf^n)\\
&=\log \prod_n \sum_\ve p(\vf^n\mid \ve)p(\ve)\\
&=\log \prod_n \sum_\ve p(\ve) \prod_{m} t_1(\vf^n_m\mid\ve_m)
\end{align*}
Here, $p(\ve)$ denotes the probability of $\ve$ according to a fixed source language model.

Once training of $\beta$ is complete, we can decode an observed target sequence $\vf$, by casting $\beta$ back in terms of $\tA$ and apply the Viterbi decoding algorithm.

To solve for $\beta$ in \eqn{eqn:biEM}, \newcite{Mylonakis2007} propose \method{bi\mbox{-}EM}; an iterative EM style algorithm in which both E- and M- steps have a closed-form solution. They do not provide the algorithm derivation. 
%we observe that the solution they proposed for the M-step does not follow from their M-step objective. 

Inspecting their M-step objective and its proposed solution, we determined that (1) the objective is not concave, hinting that a closed-form solution for the maximizer is unlikely, and (2) the proposed solution is the maximizer of a different objective function, obtained by omitting the normalization denominators in \eqn{eqn:reparam} from their M-step objective.

Despite its heuristic M-step, \method{bi\mbox{-}EM} shows improved results on both POS-tagging and monotone noun sequence translation without parallel data compared to standard EM. We compare \method{MIR} against \method{bi\mbox{-}EM} in Section \ref{sec:transliteration}.


%\subsection{Other Related Work}
%From a general machine learning perspective, our proposed method can be classified as an unsupervised regularized multi-task learning technique \cite{Evgeniou04,Caruana97}.

%\newcite{graca+alii:2010} also develop an approach that encourages bidirectional word alignment agreement, which they achieve by placing constraints on the word alignment posterior distributions. However, their optimization requires many inference calls over the parallel data, making it hard to scale.

%Beyond parallel data, $\method{MIR}$ can also be applied to large-scale MT decipherment \cite{Ravi11,Dou13}, where, so far, only a single directional model was used.
%
%Bi-directional Conversion Between Graphemes and Phonemes Using a Joint N-gram Model. \bluetext{TL: This paper discusses a \textbf{joint} grapheme-to-phoneme model, trained on parallel data -- It may be out of scope.}
%



